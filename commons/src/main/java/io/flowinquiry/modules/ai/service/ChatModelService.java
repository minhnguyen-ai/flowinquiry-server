package io.flowinquiry.modules.ai.service;

/**
 * Interface representing a generic chat model service.
 *
 * <p>This interface defines the contract for interacting with different chat models, such as OpenAI
 * or Ollama, by providing a method to process input and return a response. Implementations of this
 * interface should encapsulate the logic specific to each chat model's API or processing.
 *
 * <h3>Usage Example</h3>
 *
 * <pre>{@code
 * ChatModelService chatModelService = new OpenAiChatModelService();
 * String response = chatModelService.call("Summarize this text.");
 * System.out.println(response);
 * }</pre>
 *
 * <h3>Implementations</h3>
 *
 * <ul>
 *   <li>{@link OpenAiChatModelService}
 *   <li>{@link OllamaChatModelService}
 * </ul>
 *
 * <h3>Thread-Safety</h3>
 *
 * Implementations of this interface should ensure thread-safety if they are intended to be used in
 * concurrent environments.
 *
 * @author Hai Nguyen
 * @version 1.0
 */
public interface ChatModelService {

    /**
     * Processes the given input using the chat model and returns the model's response.
     *
     * <p>The input string can be any text to be processed by the chat model, such as a question, a
     * request for summarization, or any prompt supported by the model.
     *
     * @param input the text input to be processed by the chat model
     * @return the response generated by the chat model
     * @throws IllegalArgumentException if the input is null or empty
     */
    String call(String input);
}
